import sys
import numpy as np
import csv
class NeuralNetwork:
	def __init__(self, hidden_activation = 'relu', output_activation = 'softmax',learning_rate = 0.01, hidden_neurons = 10, output_neurons = 4):
		#initialize network
		self.learn_rate = learning_rate
		self.imageLabels = []
		self.vector = []
		self.classes = []
		self.hidden_activation = hidden_activation
		self.output_activation = output_activation
		self.hiddenLayer, self.hiddenBias = self.initialize_layer(hidden_neurons,192)
		self.outputLayer, self.outputBias = self.initialize_layer(output_neurons,hidden_neurons)

	def file_read(self, file_name):
		self.vector = []
		self.classes = []
		f = open(file_name)
		reader = csv.reader(f, delimiter = " ")
		for line in reader:
			self.imageLabels.append(line[0])
			self.classes.append(int(line[1]))
			vec = np.asarray(map(int, line[2:]))
			normalized_vector = (vec - min(vec))/ float(max(vec) - min(vec)) #normalization
			self.vector.append(normalized_vector)
			
	#initialize new layer with certain neuron count with respective inputs to neurons
	def initialize_layer(self, neuron_count, input_count):
		np.random.seed(1)
		return np.random.uniform(-0.5, 0.5, size = (neuron_count, input_count)).astype('float128') , np.ones(neuron_count)#row is nodes in layer and columns are inputs
	
	
	
	#activation function	
	
	def activation(self , x, name):
		if name == 'sigmoid':
			return 1 / (1 + np.exp(-x))
		elif name == 'tanh':
			return np.tanh(x)
		elif name == 'softmax':
			return np.exp(x) / np.sum(np.exp(x))
		elif name == 'relu':
			return x * (x > 0)
		else:
			return
	
	def activation_prime(self, x, name):
		if name == 'sigmoid':
			return x * (1 - x)
		elif name == 'tanh':
			return 1 - np.power(x, 2)
		elif name == 'softmax':
			return 1
		elif name == 'relu':
			return 1. * (x > 0)
		else:
			return
			
	def feedForward(self, X, weights, activation, bias):
		return self.activation(np.dot(X, weights.T) + bias, activation)
		
		
	def modactualouts(self, y):
		z = []
		for i in y:
			if(i == 0):
				z.append([1,0,0,0])
			elif(i == 90):
				z.append([0,1,0,0])
			elif(i == 180):
				z.append([0,0,1,0])
			elif(i == 270):
				z.append([0,0,0,1])
		return np.asarray(z)
	#train the network
	def train(self, file_name, model_file, epochs = 3000): #X = input, y = actual labels, epochs = number of iterations
		self.file_read(file_name)
		X = np.asarray(self.vector)
		y = np.asarray(self.classes)
		mody = self.modactualouts(y)
		for e in range(0,epochs):
			hidden_adjust_wts = []
			output_adjust_wts = []
			err = []
			delta_h_layer = []
			delta_o_layer = []
			print "======================EPOCH ",e+1,"=========================="
			for counter, vector in enumerate(X):

				hiddenLayerOut = self.feedForward(vector, self.hiddenLayer, self.hidden_activation, self.hiddenBias)
				outputLayerOut = self.feedForward(hiddenLayerOut, self.outputLayer, self.output_activation, self.outputBias)
				
				err.append(0.5 * np.sum((mody[counter] - outputLayerOut)**2))
				error = (outputLayerOut - mody[counter])
				
				delta_output = error * self.activation_prime(outputLayerOut, self.output_activation)
				delta_hidden = self.activation_prime(hiddenLayerOut, self.hidden_activation) * np.dot(delta_output, self.outputLayer)
				
				delta_o_layer.append(delta_output)
				delta_h_layer.append(delta_hidden)
				#self.hiddenLayer =  (self.hiddenLayer.T - (np.sum(np.array([i.T for i in np.einsum('ij,ik->ikj',X,delta_hidden)])*self.learn_rate, axis = 0)/ len(X))).T
				#self.outputLayer = (self.outputLayer - (np.sum(np.array([i for i in np.einsum('ij,ik->ikj',hiddenLayerOut,delta_output)])*self.learn_rate, axis = 0)/ len(X)))
				#print self.hiddenLayer - ((delta_hidden * vector[np.newaxis].T) * self.learn_rate).T
				#print self.outputLayer - ((delta_output * hiddenLayerOut[np.newaxis].T) * self.learn_rate).T
				hidden_adjust_wts.append(((delta_hidden * vector[np.newaxis].T) * self.learn_rate).T)
				output_adjust_wts.append(((delta_output * hiddenLayerOut[np.newaxis].T) * self.learn_rate).T)
			#weight adjustments
			len_train = len(X)
			h_wt_avg = np.sum(np.asarray(hidden_adjust_wts), axis = 0)/ len_train
			o_wt_avg = np.sum(np.asarray(output_adjust_wts), axis = 0)/ len_train
			self.hiddenLayer -= h_wt_avg
			self.outputLayer -= o_wt_avg
			#bias adjustments
			self.hiddenBias = np.sum(np.asarray(delta_h_layer), axis = 0) / len_train
			self.outputBias = np.sum(np.asarray(delta_o_layer), axis = 0) / len_train
			print ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>err: ", np.sum(np.asarray(err), axis = 0) / len(X)
			self.learn_rate = 0.01 / (1 + e / epochs)
		model = {'hiddenLayer' : self.hiddenLayer, 'outputLayer': self.outputLayer, 'hiddenBias' : self.hiddenBias, 'outputBias' : self.outputBias}
		np.save(model_file,model)

	def test(self, file_name, model_file):
		
		data = np.load(model_file)
		data = data[()]
		hiddenLayer, hiddenBias = data['hiddenLayer'], data['hiddenBias']
		outputLayer, outputBias = data['outputLayer'], data['outputBias']
		
		self.file_read(file_name)
		X = np.asarray(self.vector)
		y = np.asarray(self.classes)
		labels = self.imageLabels
		correct = 0
		for c,i in enumerate(X):
			predicted = 0
			hidden_out = self.feedForward(i, hiddenLayer, self.hidden_activation, hiddenBias)
			output_out = self.feedForward(hidden_out, outputLayer, self.output_activation, outputBias)
			if output_out.argmax() == 0:
				predicted = 0
			elif output_out.argmax() == 1:
				predicted = 90
			elif output_out.argmax() == 2:
				predicted = 180
			elif output_out.argmax() == 3:
				predicted = 270
			print labels[c]," ", predicted
			if predicted == y[c]:
				correct = correct + 1
		print "Testing Accuracy: ", (correct/float(len(y)))*100,"%"
